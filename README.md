# papers

## Offline RL
[OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning](https://openreview.net/pdf?id=V69LGwJ0lIN)

[CQL: Conservative Q-Learning for offline reinforcement learning](https://arxiv.org/pdf/2006.04779.pdf)

[Is Pessimism Provably Efficient for Offline RL?](https://arxiv.org/pdf/2012.15085.pdf)

[THE IMPORTANCE OF PESSIMISM IN FIXED-DATASET POLICY OPTIMIZATION](https://arxiv.org/pdf/2009.06799.pdf)

[Off-Policy Deep Reinforcement Learning without Exploration](https://arxiv.org/pdf/1812.02900.pdf), BCQ, ICML 2019.

[Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction](https://arxiv.org/pdf/1906.00949.pdf), BEAR, including analysis of OOD Actions in Q-Learning.


#### Model based
[MOReL: Model-Based Offline Reinforcement Learning](https://arxiv.org/pdf/2005.05951.pdf)

[MOPO: Model-based Offline Policy Optimization](https://arxiv.org/pdf/2005.13239.pdf)

[COMBO: Conservative Offline Model-Based Policy Optimization](https://arxiv.org/pdf/2102.08363.pdf)

[MODEL-BASED OFFLINE PLANNING](https://openreview.net/pdf?id=OMNB1G5xzd4) ICLR 2021, 8755

[NIPS 2020 Offline Workshop](https://offline-rl-neurips.github.io/papers.html)

[Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization](https://arxiv.org/pdf/2006.03647.pdf) NIPS 2020

### app
[OPAL: OFFLINE PRIMITIVE DISCOVERY FOR ACCELERATING OFFLINE REINFORCEMENT LEARNING](https://arxiv.org/pdf/2010.13611.pdf), Offline + hrl

[Multi-task Batch Reinforcement Learning with Metric Learning](https://proceedings.neurips.cc//paper/2020/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf), NIPS2020, Multi task.

[COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning](https://arxiv.org/pdf/2010.14500.pdf), CoRL 2020.

[KEEP DOING WHAT WORKED:BEHAVIOR MODELLING PRIORS FOR OFFLINE REINFORCEMENT LEARNING] (https://arxiv.org/pdf/2002.08396.pdf) ICLR 2020.

[Q-Value Weighted Regression: Reinforcement Learning with Limited Data](https://arxiv.org/pdf/2102.06782.pdf#page=10&zoom=100,0,0) An extension of [KEEP DOING WHAT WORKED ...], unaccepted. 

## OPE
[AUTOREGRESSIVE DYNAMICS MODELS FOR OFFLINE POLICY EVALUATION AND OPTIMIZATION](https://openreview.net/pdf?id=kmqjgSNXby), ICLR 2021.

[BENCHMARKS FOR DEEP OFF-POLICY EVALUATION](https://arxiv.org/pdf/2103.16596.pdf), ICLR 2021.

## IRL
[GAIL- Generative Adversarial Imitation Learning](https://papers.nips.cc/paper/2016/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf)

## Sim2Real
[Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model](https://arxiv.org/pdf/1610.03518.pdf), CoRR, 2016
propose a method to combine an inverse dynamics policy learned with expert demonstrations and a simulator policy trained in simulator.

[Offline Imitation Learning with a Misspecified Simulator](https://proceedings.neurips.cc/paper/2020/file/60cb558c40e4f18479664069d9642d5a-Paper.pdf), Nips, 2020

[Virtual-Taobao: Virtualizing Real-world Online Retail Environment for Reinforcement Learning](https://arxiv.org/pdf/1805.10000.pdf)

## IL
[Error Bounds of Imitating Policies and Environments](https://papers.nips.cc/paper/2020/file/b5c01503041b70d41d80e3dbe31bbd8c-Paper.pdf)
The paper analyzes the value gap between the expert policy and imitated policies by two imitation methods, behavioral cloning and generative adversarial imitation. The results support that generative adversarial imitation can reduce the compounding errors compared to behavioral cloning, and thus has a better sample complexity.



## HRL
[Between MDPs and semi-MDPs:A framework for temporal abstractionin reinforcement learning](https://reader.elsevier.com/reader/sd/pii/S0004370299000521?token=6F22DD36E1D8394D262562BDA941EB802AF20CADF56CC8383CACBC8E83A27EFC1D7F8A5F169F49FE35CC8E50DEB3319E) Sutton, 1999, option framework

## ML
### Pareto

[Pareto Multi-Task Learning](https://arxiv.org/pdf/1912.12854.pdf)

[Multi-Objective Reinforcement Learning using Sets of Pareto Dominating Policies](https://jmlr.csail.mit.edu/papers/volume15/vanmoffaert14a/vanmoffaert14a.pdf), 2014, JMLR,Pareto in RL. 

[Bridging Theory and Algorithm for Domain Adaptation](https://arxiv.org/pdf/1904.05801.pdf), domain transferï¼Œ MMD.

